\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{packages/Comments}
\input{packages/Comments}
\input{packages/Reflection}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}
There are multiple software components that are required for us to complete our end goal, as previously mentioned in SRS, we need to test each of these components to ensure accuracy and reliability in our product. More of the specifics can be found below.  
\subsubsection{User Interface (UI)}
\textbf{Function}: Ensure that the interface is easy to navigate for users like radiologists, doctors, or technicians. This includes testing for intuitive layouts, clear labels, and smooth workflows.
\begin{itemize}
    \item \textbf{Functionality}: Check if every UI element this includes: buttons, drop downs, the menu and ensure the software front ended system responds correctly to user input.
    \item \textbf{Performance}: Verify the UI’s responsiveness and loading times, particularly when displaying large medical images like X-rays.
\end{itemize}

\subsubsection{Image Preprocessing and Enhancement}
\textbf{Function}: Before analysis, the X-ray images undergo preprocessing steps to enhance image quality and ensure consistent inputs for the AI model.
\begin{itemize}
    \item \textbf{Accuracy of Noise Reduction}: Removes artifacts and noise from X-rays, improving image clarity. This is usually done by making the image grey scale or removing color from the image to ensure further accuracy when reading the image. 
    \item \textbf{Normalization}: Standardizes the pixel values across images to ensure uniformity, allowing the AI model to better detect subtle differences.
\end{itemize}

\subsubsection{Processing The Image}
\textbf{Function}: The AI model reads the image and looks at the features or patterns from the X-ray images that are indicative of various lung conditions.
\begin{itemize}
    \item \textbf{Accuarcy of Texture and Shape Analysis}: System analyzes shape and density of lung tissues by looking at tensor values and identifying anomalies.
\end{itemize}
\subsubsection{Disease Classification}
\textbf{Function}: Once features are extracted, using AI classification system categorizes pattern  into different diseases; returning the probabilities.
    \begin{itemize}
        \item \textbf{Accuracy and speed of Disease Models}: Model is trained on large datasets of labeled X-ray images specifically CheXRPT, aiding it to recognize the patterns associated with diseases, this includes: Pneumonia, Lung Cancer, Tuberculosis and more.
        \item \textbf{Accuracy  of Probability Scores}: It provides a probability or confidence score for each disease, indicating the likelihood of the condition being present.
    \end{itemize}
    \textbf{Function}: Integrate our application hospital systems and hospital imaging software  for seamless data flow.
    \begin{itemize}
            \item \textbf{Functionality of Automated Reporting}: Upon completing the analysis, the software can generate the report showing the probabilities of diseases and save that information onto the patients hospital data or user data of that hospital.
    \end{itemize}
    \subsection{Objectives}

\subsubsection{Ensure Software Accuracy or Correctness (In scope)}
\begin{itemize}
    \item The primary objective of this project is to build confidence in the correctness of the AI-powered diagnostic tool for lung diseases. 
    \item The software should be able to consistently and accurately predict the with a score of a minimum of 80$\%$ accuracy. 
    \item This probability will be derived from the model estimating the odds of  various lung conditions for example,  pneumonia or lung cancer from X-ray images. 
    \item Verifying the correctness of the AI's predictions is essential to ensure reliable clinical decision-making.
\end{itemize}
\subsubsection{Demonstrate Adequate Usability (In scope)}
\begin{itemize}
    \item Given that medical professionals, such as radiologists, students, and doctors, will use the tool, the UI needs to be intuitive and efficient. 
    \item The focus will be on ensuring the tool’s interface is easy to navigate, with clear access to essential features such as uploading X-rays, viewing probabilities, and generating reports.
    \item This probability will be derived from the model estimating the odds of  various lung conditions for example,  pneumonia or lung cancer from X-ray images. 
    \item  The goal of the application is to ensure correct results and not create an inner challenge to use the software itself, so users can quickly perform key tasks.
\end{itemize}
\subsubsection{Security of Patient Data (In scope)}
\begin{itemize}
    \item Since the software will handle sensitive patient information, it must comply with privacy regulations such as PIPEDA for Canada or HIPAA if used in the US. 
    \item This objective will focus on verifying that patient data is securely stored and transferred, and that access control mechanisms are in place to protect against unauthorized use.
\end{itemize}

\subsubsection{Verification of External Libraries and APIs (Out of scope)}
\begin{itemize}
    \item  The system relies on pre-existing libraries and APIs for tasks such as image processing, patient data handling, and possibly pre-trained AI models.
    \item The quality of these external dependencies will not be verified in this project.
    \item Our team believes that the External libraries and APIs will be assumed to have been tested by their implementation teams.
\end{itemize}
\subsubsection{Extensive Usability Testing (Out of scope)}
\begin{itemize}
    \item Ensuring we do adequate usability testing is a priority, however going further than that is a  problem for the longer future as we have limited resources, only basic usability testing will be done to ensure the software meets general usability standards. 
\end{itemize}
\subsubsection{European Regulatory Compliance (Out of scope)}
\begin{itemize}
    \item The project will not focus on ensuring compliance with European health data privacy regulation, for example, the General Data Protection Regulation in the European Union. As we are currently only looking to launch this application for North America more specifically Canada and the United States. 
\end{itemize}

\subsection{Challenge Level and Extras}

\subsubsection{Challenge Level}
My team and I believe the challenge level for this project to be advanced. This classification is based on the complexity of integrating AI for diagnostic purposes, the necessity for robust data handling and privacy compliance, and the need to develop a user-friendly interface for medical professionals. The project requires a solid understanding of machine learning principles, software development practices, and compliance with health data regulations. Our knowledge of Tensors, Linear Algebra, and machine learning, and image processing will be required for this application to release and work comprehensively. 

\subsubsection{Extras}
As mentioned in SRS and in the previous sections: Basic usability testing will be conducted to ensure that the user interface is intuitive and easy for healthcare professionals to navigate. This will include gathering feedback from potential users to identify any usability issues and make necessary adjustments.

\subsection{Relevant Documentation}

The two main relevant documentation that helped guide us in System Verification and Validation Plan (VnV) was Software Requirements Specification (SRS) and development plan. SRS was crucial for the successful development of the AI-powered diagnostic tool for lung diseases, as it clearly outlines the software’s intended functionality, performance requirements, and design constraints, providing a roadmap for us developers and aiding us understand our stakeholders. The problem statement identifies the specific challenges to be addressed, while the development plan documented helped us by outlining what approach and resources were needed to achieve our project goals. These documents are very relevant as they helped ensure that the SRS aligns with the project’s objectives, guiding the VnV process by defining acceptance criteria, enabling the creation of test cases linked to specific requirements, and establishing a traceability matrix to ensure all functionalities are tested. Additionally, the SRS aids in managing changes, identifying risks, and ensuring compliance with regulations like PIPEDA or HIPAA, ultimately enhancing quality assurance practices and helping to deliver a reliable and effective software product.



\citet{SRS}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}

\subsubsection{Understanding Medical Conditions}
\textbf{Assigned Team Members: All} \\\\
For the verification process to ensure that the AI accurately detects lung diseases in chest X-rays, all team members must gain a deep understanding of the relevant medical conditions. This is essential for assessing whether the AI outputs align with real-world clinical expectations. To achieve this, the team needs to thoroughly review annotated chest X-ray datasets and medical literature. Without this foundational knowledge, the team cannot effectively validate the model’s diagnostic accuracy. During verification, each member must be capable of recognizing potential errors in the model’s predictions and correlating them with known medical indicators, ensuring the AI delivers clinically relevant results.
\subsubsection{Learning PyTorch Fundamentals}
\textbf{Assigned Team Members: Patrick, Reza, and Kelly}\\\\
To achieve high-quality verification of the AI model, the team must master PyTorch’s core functionalities, particularly related to the data flow and training process. The team needs to understand how to verify that the model is not just functioning, but also learning and improving in a meaningful way. This includes using PyTorch’s tensor operations and autograd features to test whether gradients are being calculated and applied correctly during backpropagation. Additionally, the team must verify that the preprocessing steps—such as resizing, normalizing, and augmenting X-ray images—are executed properly, as incorrect data handling could introduce significant errors during model training. High-quality verification requires confirming that all data transformations support the model’s learning objectives and that the AI is robust across different image variations.
\subsubsection{Data Visualization Analysis}
\textbf{Assigned Team Members: Ayman and Nathan}\\\\ 
For effective verification, the team must ensure that data visualizations clearly reflect the AI model’s performance, making it easier to spot anomalies or trends that might indicate issues. Ayman needs to focus on developing visuals that accurately represent both the training progress and the diagnostic results from the AI model. The team must verify that these visualizations allow for easy comparison between predicted outcomes and the actual clinical diagnoses. Ensuring that the plots are interpretable and correctly display model metrics (e.g., accuracy, loss over time, false positives/negatives) is key to validating the AI’s reliability. Without this verification, it would be difficult to pinpoint areas where the model may require refinement.


\subsubsection{Web Creation}
\textbf{Assigned Team Members: Ayman and Nathan} \\\\
To meet the high standards for user experience and functionality, the Nathan and Ayman must rigorously verify that the frontend interface is not only visually appealing but also fully functional and responsive. Nathan needs to verify that the UI design facilitates a seamless interaction with the AI model, ensuring that healthcare professionals can input data and interpret results with ease. It is critical to validate the integration between the frontend and backend, ensuring that data is accurately retrieved, processed, and displayed without errors. Usability testing must be a priority during verification, as any gaps in the user interface could hinder the adoption of the tool by medical professionals. Ensuring accessibility and performance across various devices is also essential to meet the project’s quality standards.

\subsubsection{Users of the Application}
\textbf{Assigned Team Members: All} \\\\
Understanding user expectations is critical for verifying that the final product meets the needs of its primary users—healthcare professionals. The team must ensure that the AI model's outputs are not only accurate but also presented in a format that healthcare users find intuitive and actionable. Verification must include gathering feedback from potential users to assess whether the AI-generated diagnostics align with clinical workflows. Additionally, the team needs to verify that the interface provides relevant and precise information without overwhelming the user with unnecessary details. Ensuring that the tool is user-friendly and caters to both specialists and general practitioners is key to validating its practicality and effectiveness in real-world scenarios.


\subsection{SRS Verification Plan}
\subsection{SRS Verification Plan}
\subsubsection{Peer Review}
\begin{itemize}
    \item \textbf{Ad Hoc Feedback:} We will conduct peer reviews where classmates and our primary reviewer provide feedback. 
    \item \textbf{Creating Issues:} Each team member will be responsible for reading the SRS, evaluating its clarity, and checking for missing requirements and for anything missing that team member will required to make a github issue entailing that we need to look to add or fix a section of SRS. 
    \item \textbf{Peer reviews:} Supervisor, stakeholders, and other peers will read our the document and give use feedback by doing this we can ensure its understandable to all stakeholders, especially those unfamiliar with the project.
    \item \textbf{Pre-Meeting Preparation:} Before meeting with the supervisor, the team will ensure that the SRS is updated and ready for review. We would this by going through the rubric and creating a checklist. This checklist would include: Questions like are all the functional and non-functional requirements included? Are the constraints in scope or outside scope constraints are they truly constraints. Making a checklist with questions similar to these will help verify that the project is coherent and comprehensive.
    \item \textbf{Post-Meeting Notes:} we will take notes on what the supervisor or TA told us and ensure that we look to add or fix whatever was mentioned during the meeting. This will be another checklist of things we must fix not like the previous checklist that is primarily focused on asking questions. 
    \item \textbf{Questions for the Supervisor $\&$ TA's:} We will ask key questions that would focus on verifying the scope, clarity, and feasibility of our SRS document. An example, could be asking if the NFRs or non-functional requirements align with the project’s quality objectives.
\end{itemize}

\subsection{Design Verification Plan}

The design verification plan outlines the strategies and procedures that the team will use to verify the correctness and reliability of the design of our X-ray analysis and disease prediction application. This plan will serve as a guideline during the testing phase to ensure that the design meets the intended requirements and can mitigate potential risks identified by the team. The following procedures will be undertaken by the testing team during the verification process:

\subsubsection{Document Review}
The system's design documentation and related materials will be reviewed by each member of the testing team after the initial draft is produced. During the document review process, the testing team will:
\begin{itemize}
\item Ensure that the design of the system aligns with all functional and non-functional requirements, particularly those related to medical accuracy, data security, and regulatory compliance.
\item Assess if the documentation accurately describes the intended functionalities and behavior of the system, including its ability to analyze X-rays, classify diseases, and provide accurate predictions. Any discrepancies between the design and requirements should be recorded and discussed further.
\end{itemize}

\subsubsection{Prototype Testing}
After the scheduled Proof of Concept (POC) demonstration, a prototype for each major system component should be completed. The prototypes will be assembled for system testing. During this phase, the testing team will:
\begin{itemize}
\item Verify the application’s ability to process X-ray images accurately and consistently.
Assess system performance under different load conditions, such as batch processing multiple images simultaneously.
\item Test the robustness of disease prediction algorithms under various scenarios, including rare and complex cases.
\item Evaluate the application's user interface for healthcare professionals, ensuring it is intuitive, provides clear predictions, and integrates well with clinical workflows.
Simulate failure conditions to verify error-handling mechanisms, such as incomplete data, corrupted files, or system downtime.
\end{itemize}


\subsubsection{Design Verification Checklist}

\textbf{Document Review}
    \begin{itemize}
        \item Ensure design aligns with all functional and non-functional requirements.
        \item Confirm documentation accuracy for image processing, classification, and prediction.
        \item Record any discrepancies from requirements.
    \end{itemize}
\textbf{Prototype Testing}
\begin{itemize}
    \item \textbf{System Performance}
    \begin{itemize}
        \item Verify accurate X-ray image processing.
        \item Test performance under various load conditions.
        \item Check response time for predictions.
        \item Ensure scalability for future usage.
    \end{itemize}
\item \textbf{Disease Prediction Accuracy}
    \begin{itemize}
        \item Test accuracy with a benchmark dataset.
        \item Evaluate algorithm on rare and complex cases.
        \item Check consistency of predictions across runs.
    \end{itemize}

    \item \textbf{User Interface (UI)}
    \begin{itemize}
        \item Confirm UI is intuitive for healthcare professionals.
        \item Ensure clear display of disease predictions.
        \item Verify data visualization tools are easy to interpret.
        \item Check seamless integration with clinical systems.
    \end{itemize}
\end{itemize}
\subsection{Verification and Validation Plan Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.


\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}