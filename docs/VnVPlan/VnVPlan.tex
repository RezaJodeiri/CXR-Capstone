\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of 
limitations in your resources for verification and validation.  You can't do everything, 
so what are you going to prioritize?  As an example, if your system depends on an 
external library, you can explicitly state that you will assume that external library 
has already been verified by its implementation team.}

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

\citet{SRS}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
  primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
read over the SRS.  You should explain your structured approach to the review.
Will you have a meeting?  What will you present?  What questions will you ask?
Will you give them instructions for a task-based inspection?  Will you use your
issue tracker?}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}


This section contains tests for the Non-Functional Requirements. The subsections of this part were created based on the subsections of Non-functional Requirements listed in the SRS. Each test was created based on the Fit Criterion of the requirements that they covered. 
\subsection{Look and Feel}
\textbf{NFR-LF1} \textbf{Type} Non-Functional Dynamic Manual\\
         \indent {\textbf{Initial State}: The system must be installed and accessible.}\\
         \indent {\textbf{Input / Condition}: Adjust window/level settings. Navigate between over tabs.Apply basic tools (zoom, pan, measure). Export or save images from the viewer.}\\
         \indent {\textbf{Output}: The UI layout should be similar to standard medical imaging software (with menu positions, toolbar layout, and terminology consistent with common practices).}\\
         \indent {\textbf{How test will be performed}: Ensure the system under test and reference medical imaging software are installed and configured. Launch the system under test and the reference software side-by-side. Compare the menu structure, toolbars, icons, panels, and overall layout. Verify whether the placement of key elements matches standard layouts. Review menu labels, tooltips, and buttons to ensure they use medical imaging terms. Adjust window/level settings. Use zoom, pan, and measurement tools.}\\
        \\
\textbf{NFR-LF2} \textbf{Type} Non-Functional Dynamic Manual \\
        \indent \textbf{Initial State}: The system under test is installed and accessible on the target device. The monitor brightness is set to a comfortable level to ensure consistency during the test.\\
        \indent \textbf{Input / Condition}: User Interaction with the interface for typical workflows.\\
        \indent \textbf{Output}: color contrast ratio (is 4.5:1 or higher for text). Font sizes at least 12-14 points for normal text. \\
        \indent \textbf{How test will be performed}: Launch the system and ensure the default color schemes and fonts are applied. Perform tasks continuously for 1-2 hours while interacting with different parts of the system interface (e.g., forms, menus). Check for signs of fatigue or frustration (e.g., rubbing eyes, slowing down). Use tools to measure the color contrast ratio.\\
        \\
\subsection{Usability and Humanity}
\noindent \textbf{NFR-UH1} \textbf{Type} Non-Functional Dynamic Automated \\
        \indent \textbf{Initial State}: User accounts and access credentials are prepared for all healthcare professionals participating in the test.Any required files or images are pre-loaded and accessible for the tasks.\\
        \indent \textbf{Input / Condition}: Automated script with a list of specified tasks and user accounts\\
        \indent \textbf{Output}: Percentage of user accounts that successfully completed all tasks without assistance: $\text{Success Rate} = \frac{\text{Number of Successful Participants}}{\text{Total Number of Participants}} \times 100$ \\
        \indent \textbf{How test will be performed}: The automated script will perform the following tasks with each user account and calculate the success rate: Upload a Chest X-ray image to the system. View analysis results or processed reports. Adjust image settings (e.g., window/level, zoom). Export the analysis or report to a local folder. \\ 
\\
\textbf{NFR-UH2} \textbf{Type} Non-Functional Dynamic Automated \\
        \indent \textbf{Initial State}: List of user accounts and access credentials are prepared. Full access to the user interface. Interface elements (buttons, icons, menus) are accessible for interaction.\\
        \indent \textbf{Input / Condition}: Automated script with a list of interactions specified to interact with the tabs / other listed functionalities.\\
        \indent \textbf{Output}: Log whether tool-tips and help content were displayed correctly for each element tested.\\
        \indent \textbf{How test will be performed}: The test script will detect if the tool-tip appears after a short delay (0.5-1 second) when it hovers over an element. When it clicks on a help icon (e.g., “?” symbol) or help link, the test script checks if the system provides relevant information about that part of the interface. When the test script finished, it will provide a log including if tool-tips and help content were displayed correctly for each element tested. \\
\subsection{Performance}
\noindent \textbf{NFR-PR1} \textbf{Type} Non-Functional Dynamic Automated \\
        \indent \textbf{Initial State}: User is logged in. Standard chest X-ray images are available for testing in PNG/JPG format (depending on system requirements). \\
        \indent \textbf{Input / Condition}: Automated test script interacts with the system.\\
        \indent \textbf{Output}: Percentage of the number of the chest X-ray image that are analyzed correctly.\\
        \indent \textbf{How test will be performed}: The test script will upload a X-ray image through the interface. It will monitor the system to ensure it processes and analyzes the image. The moment the results or report are displayed or the system indicates completion, it checks the timing (<30 seconds). The process is repeated multiple times (at least 5 trials) to account for variations in processing time. \\
\\
\textbf{NFR-PR2} \textbf{Type} Non-Functional Dynamic Automated \\
        \indent \textbf{Initial State}: A monitoring system is set up to log availability metrics, such as downtime events, mean time to recovery, and up-time percentage.\\
        \indent \textbf{Input / Condition}: Server outages / network issues / high number of concurrent users.\\
        \indent \textbf{Output}: Up-time Percentage; Downtime Events (Log the number, duration, and cause of any disruptions or downtime.) Mean Time to Recovery.\\
        \indent \textbf{How test will be performed}: Two Automated scripts will start the monitoring system for a defined period (e.g., 24 hours or 1 week). One will simulate server outages or network issues by disabling a service temporarily and record the downtime, mean time to recovery. The other will use load testing tools (like Apache JMeter or Locust) to simulate a high number of concurrent users during peak hours and monitor response times. Two scripts will combine their results and return the output once they all finished.\\
\\
\textbf{NFR-PR3} \textbf{Type} Non-Functional Dynamic Automated\\
        \indent \textbf{Initial State}: All dependencies (database, network, storage, and image processing engine) are configured and operational. Monitoring tools are available to track system performance, including: CPU and memory usage, Image processing times.\\
        \indent \textbf{Input / Condition}: Collections of 20 identical or varied images, multiple user accounts. Baseline for each image processing is 20 seconds\\
        \indent \textbf{Output}: Processing Time(Float), System Resource Utilization(Float), Images uploading success / fail (Boolean)\\
        \indent \textbf{How test will be performed}: The automated script will simulate different users to upload 20 images simultaneously and start the stopwatch or monitoring tool when the uploads begin. It will keep track of the CPU, memory, and I/O usage during the processing of the 20 images. When images processing finished, it will calculate and return the average processing time for the 20 images, return CPU usage, memory usage and check for errors or failures.\\
        \\
\subsection{Operational and Environmental}
\textbf{NFR-OE1} \textbf{Type} Non-Functional Dynamic Manual\\
        \indent \textbf{Initial State}: The AI system and the PACS are both connected to the same hospital network and the DICOM configuration  for both systems is correctly set.\\
        \indent \textbf{Input / Condition}: AI-processed results including annotations or diagnostic report as an image (in DICOM format). Metadata with patient ID (string) and instance number (string).\\
        \indent \textbf{Output}: Boolean indicating whether the PACS successfully stores the AI-processed results with correct format (annotated image or report). The stored result is associated with the correct patient ID and instance number in the PACS.\\
        \indent \textbf{How test will be performed}: Automated script will processes the retrieved chest X-ray image and generates an annotated image or diagnostic report in DICOM format. It will then send the annotated result to the PACS and verify the response from the PACS to ensure that the transfer was successful Then it will log into the PACS and search for the patient ID to confirm that the stored result is associated with the correct instance number. The automated scipt can be run multiple times depending on the need. \\
        \\
\textbf{NFR-OE2} \textbf{Type} Non-Functional Dynamic Automated\\
        \indent \textbf{Initial State}: The network is configured normally with minimal latency and no packet loss initially. Necessary patient data and chest X-ray studies are available for retrieval.\\
        \indent \textbf{Input / Condition}:Retrieve and process a chest X-ray image, Store the processed result, and send the data to external.\\
        \indent \textbf{Output}: Boolean indicating if latency is within a reasonable time (190ms~220ms), logs include latency and packet loss statistics.\\
        \indent \textbf{How test will be performed}: The automated script will simulate Network Latency and Packet Loss using a network emulation tool. It will then processes the retrieved chest X-ray image and generates an annotated image or diagnostic report, meanwhile monitoring the time taken for operations and the number of retries and checking the system logs for any warnings or errors related to packet loss or latency. It will check if the system raises alerts if packet loss exceeds 1% or if latency impacts performance significantly and return the results.
        \\
        \\
\subsection{Security and Privacy}
\noindent \textbf{NFR-SR1} \textbf{Type} Non-Functional Dynamic Automated\\
         \indent \textbf{Initial State}: AES-256 encryption libraries (such as OpenSSL or Cryptography in Python) are configured for the system. Patient data, (including DICOM images and reports), is stored on the system or transmitted over the network.\\
         \indent \textbf{Input / Condition}:  X-ray image and diagnostic report with (Patient ID and instance number). Secret key and initialization vector (IV) for AES-256 encryption.\\
         \indent \textbf{Output}: All patient data, including images and reports, is encrypted using AES-256 both during storage and transmission.\\
         \indent \textbf{How test will be performed}: The automated system will trigger the AI system to retrieve a chest X-ray image and process it. It will check if transmitted data (images and reports) appears as encrypted bytes and no readable patient information or image data should be visible. After processing, the AI system stores the data and the automated script will try to open a stored DICOM image or report directly without using the decryption mechanism, which should appear as unreadable, encrypted content (random bytes). When AI system decrypt and access the stored image or report, the automated script will detect if the decrypted data matches the original data.\\
         \\
\textbf{NFR-SR2} \textbf{Type} Non-Functional Dynamic Automated\\
        \indent \textbf{Initial State}: The AI system has different user accounts with different assigned roles. Each role has specific permissions defined.\\
        \indent \textbf{Input / Condition}: Role 1: Radiologist: Can view diagnostic results and patient images. Role 2: Administrator: Can manage user accounts and system configurations. (user\underline radiologist, user\underline admin)\\
        \indent \textbf{Output}: Radiologist should be able to view diagnostic reports and images and should not be able to manage user accounts, Administrator should be able to create and manage user accounts and should not have access to patient diagnostic reports.\\
        \indent \textbf{How test will be performed}: The automated script will login as either one of the user (user\underline radiologist or user\underline admin). For example, the automated script logged in user\underline radiologist and views diagnostic reports and download patient images, which should be successful, if not the system will terminated. It will then attempt to manage user accounts which the access should be denied. Same process is repeated for  user\underline admin.\\
        \\
\subsection{Maintainability and Support}
\textbf{NFR-MS1} \textbf{Type} Non-Functional Static Manual\\
        \indent \textbf{Initial State}: The AI system code-base is deployed in a version-controlled environment. Documentation for each module (e.g., README files, API references, inline comments) is available in the repository.\\
        \indent \textbf{Input / Condition}: All modules within the code repository is accessible.\\
        \indent \textbf{Output}: All key functionalities (e.g., data ingestion, inference, reporting) are encapsulated in separate, well-defined modules. Modules can function independently with minimal coupling. Dependencies between modules are well-documented.\\
        \indent \textbf{How test will be performed}: Test person should clone the repository of this AI system and identify and list all modules, review dependencies to ensure minimal coupling and make sure each module is self-contained and can be modified without breaking other parts of the system.\\
        
\noindent \textbf{NFR-MS2} \textbf{Type} Non-Functional Dynamic Automated\\
        \indent \textbf{Initial State}: Code repository contains the entire AI system, automated testing framework (e.g., JUnit, Pytest)  is installed and configured in the project. Tests include unit tests, integration tests, system tests and end-to-end tests. Code coverage tool (e.g., coverage.py, JaCoCo) is integrated.\\
        \indent \textbf{Input / Condition}: Unit tests, integration tests, and end-to-end tests.\\
        \indent \textbf{Output}: Code coverage report.\\
        \indent \textbf{How test will be performed}: The automated test script will navigate to the project directory and execute the automated test suite using the testing framework (e.g., Pytest or JUnit).\\
\subsection{Cultural}
\noindent \textbf{NFR-CR1} \textbf{Type} Non-Functional Dynamic Manual\\
        \indent \textbf{Initial State}: The language options (English and French) are available in the settings menu.\\
        \indent \textbf{Input / Condition}: Switch language between English and French. Generate report with English and French. \\
        \indent \textbf{Output}: No untranslated or misaligned content should appear.\\
        \indent \textbf{How test will be performed}: Person conducting the test will switch the language from English to French (and vice vera) in the settings menu and inspect Menus, Buttons/Labels, Error Messages and Notifications and other necessary elements for correctness. The test person will thenUse the system to generate a diagnostic report in English (and French) for a chest X-ray image. He / She will review the report for medical findings and patient info to verify that both reports contain the same findings but in the correct language. He / She will also trigger an error condition (e.g., entering invalid patient data) to verify that the error message appears in the selected language (English or French).\\
        \\
\subsection{Legal}
\noindent \textbf{NFR-LR1} \textbf{Type} Non-Functional Static Manual\\
        \indent \textbf{Initial State}: HIPAA and PIPEDA documentation are in place, development artifacts and source code are available. \\
        \indent \textbf{Input / Condition}: Development artifacts, system design documentation.\\
        \indent \textbf{Output}: System design aligns with the requirements of HIPAA (US) and PIPEDA (Canada). If not, all software risks are identified, evaluated, and mitigated.\\
        \indent \textbf{How test will be performed}: Compliance audits will verify the system design and related documentation, to see if the AI system meets data protection and regulation requirements.\\
    \\

\noindent \textbf{NFR-LR2} \textbf{Type} Non-Functional Static Manual\\
        \indent \textbf{Initial State}: ISO 13485 documentation are in place, development artifacts are available. (verification and validation plans, risk analysis reports, requirements, design documents, test plans etc.)\\
        \indent \textbf{Input / Condition}: Development artifacts, processes(Software lifecycle management process, design review process etc).\\
        \indent \textbf{Output}: All development processes align with the requirements of ISO 13485 and if not, all software risks are identified, evaluated, and mitigated.\\
        \indent \textbf{How test will be performed}: The person conducting the test will first review the development artifacts and processes to confirm that: development processes follow ISO 13485 guidelines and risk management procedures are aligned with ISO 14971. He / She will verify that requirements documents are available and cover all key system functionalities, and the system shall adhere to the ISO 13485 standard for medical device software development.\\
\\
\subsection{Health and Safety}
\textbf{NFR-HS1} \textbf{Type} Non-Functional Dynamic Manual\\
        \indent \textbf{Initial State}: User roles are configured within the system, including Radiologist, Clinician, etc.\\
        \indent \textbf{Input / Condition}: AI-generated report, Radiologist User Account\\
        \indent \textbf{Output}: Logs including radiologist’s review action (confirm/reject) with a timestamp.\\
        \indent \textbf{How test will be performed}: The test person will login using a Radiologist User Account and upload a test chest X-ray image to the AI system. The AI system analyzes the image and generates a report (flagged as preliminary). The test person will then access the preliminary report through the system’s dashboard and confirm / reject the diagnosis. If confirmed, the report should be marked as final and made available to clinicians / patients. If rejected: The system should log the feedback and trigger a re-analysis request. \\
    \\

\noindent \textbf{NFR-HS2} \textbf{Type} Non-Functional Dynamic Manual\\
        \indent \textbf{Initial State}: The AI system is deployed and accessible to radiologists, clinicians, and other users.\\
        \indent \textbf{Input / Condition}: Access the AI-generated diagnostic report and view the user interface displaying AI results.\\
        \indent \textbf{Output}: Disclaimer is prominently displayed and easy to read. \\
        \indent \textbf{How test will be performed}: The test person should generate a generate a diagnostic report using the AI system and open the generated report. He / She should check if the disclaimer is present on the report, either at the top, bottom, or in a prominent section, and the disclaimer test matches the predefined wording. He / She should also access the user interface which display AI results and select a patient report, check if the disclaimer is clearly displayed alongside the AI result and cannot be dismissed. \\

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\begin{tabular}{|c|cc|cc|ccc|cc|cc|cc|c|cc|cc|}
  \hline
  \multicolumn{1}{|c|}{NFR ID} & \multicolumn{2}{c|}{LF} & \multicolumn{2}{c|}{UH} & \multicolumn{3}{c|}{PR} & \multicolumn{2}{c|}{OE} & \multicolumn{2}{c|}{SR} & \multicolumn{2}{c|}{M} & CR & \multicolumn{2}{c|}{LR} & \multicolumn{2}{c|}{HS} \\ 
  \hline
  & 1 & 2 & 1 & 2 & 1 & 2 & 3 & 1 & 2 & 1 & 2 & S1 & S2 & 1 & 1 & 2 & 1 & 2 \\ 
  \hline
  LF 1 & x &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  LF 2 &  & x &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  UH 1 &  &  & x &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  UH 2 &  &  &  & x &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  PR 1 &  &  &  &  & x &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  PR 2 &  &  &  &  &  & x &  &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  PR 3 &  &  &  &  &  &  & x &  &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  OE 1 &  &  &  &  &  &  &  & x &  &  &  &  &  &  &  &  &  &  \\ 
  \hline
  OE 2 &  &  &  &  &  &  &  &  & x &  &  &  &  &  &  &  &  &  \\ 
  \hline
  SR 1 &  &  &  &  &  &  &  &  &  & x &  &  &  &  &  &  &  &  \\ 
  \hline
  SR 2 &  &  &  &  &  &  &  &  &  &  & x &  &  &  &  &  &  &  \\ 
  \hline
  MS1 &  &  &  &  &  &  &  &  &  &  &  & x &  &  &  &  &  &  \\ 
  \hline
  MS2 &  &  &  &  &  &  &  &  &  &  &  &  & x &  &  &  &  &  \\ 
  \hline
  CR 1 &  &  &  &  &  &  &  &  &  &  &  &  &  & x &  &  &  &  \\ 
  \hline
  LR 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  & x &  &  &  \\ 
  \hline
  LR 2 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & x &  &  \\ 
  \hline
  HS 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & x &  \\ 
  \hline
  HS 2 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & x \\ 
  \hline
  \caption{Non-functional requirements traceability}
  \label{tab:nf_requirements}
  \end{tabular}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}